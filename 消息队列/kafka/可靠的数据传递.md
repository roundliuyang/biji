# 可靠的数据传递

详细可见：Kafka权威指南第七章：可靠的消息传递





## 可靠性保证

ACID 大概是大家最熟悉的一个例子，它是关系数据库普遍支持的标准可靠性保证。ACID 指的是原子性、一致性、隔离性和持久性。如果一个数据库厂商说他们的数据库遵循 ACID 规范，那么就是在说他们的数据库可以保证事务行为具有事务性。
有了这些`保证`，我们才能相信在应用程序中使用关系数据库是安全的——我们知道它们给出了哪些承诺，也知道它们在不同环境条件下会表现出怎样的行为。我们了解这些保证机制，并会基于这些保证机制开发安全的应用程序。
所以，了解系统的保证机制对构建可靠的应用程序来说至关重要，这也是在不同的环境条件下解释系统行
为的前提。那么 Kafka 可以在哪些方面做出保证呢？



- Kafka 可以保证分区中的消息是有序的。如果使用同一个生产者向同一个分区中写入消息，并且消息

  B 在消息 A 之后写入，那么 Kafka 可以保证消息 B 的偏移量比消息 A 的偏移量大，而且消费者会先
  读取消息 A 再读取消息 B。

- 一条消息只有在被写入分区所有的同步副本时才被认为是“已提交”的（但不一定要冲刷到磁盘上）。
  生产者可以选择接收不同类型的确认，比如确认消息被完全提交，或者确认消息被写入首领副本，或
  者确认消息被发送到网络上。

- 只要还有一个副本是活动的，已提交的消息就不会丢失。

- 消费者只能读取已提交的消息。

在构建系统时，借助这些基本的保证机制可以提升系统可靠性，但它们本身并不能让系统完全可靠。构建
一个可靠的系统需要做出一些权衡，Kafka 为管理员和开发者提供了一套配置参数，他们可以控制这些参
数以获得想要的可靠性。这是消息存储可靠性和一致性的重要程度与可用性、高吞吐量、低延迟和硬件成
本的重要程度之间的一种权衡。



## 复制

Kafka 的复制机制和分区多副本架构是 Kafka 可靠性保证的核心。把消息写入多个副本可以保证 Kafka 在发生崩溃时仍然能够提供消息的持久性。
第 6 章已经深入解释过 Kafka 的复制机制，现在重新回顾一下主要内容。

Kafka 的主题会被分为多个分区，分区是最基本的数据构建块。分区存储在单个磁盘上，Kafka 可以保证分区中的事件是有序的。一个分区可以在线（可用），也可以离线（不可用）。每个分区可以有多个副本，其中有一个副本是首领。所有的事件都会被发送给首领副本，通常情况下消费者也直接从首领副本读取事件。其他副本只需要与首领保持同步，及时从首领那里复制最新的事件。当首领副本不可用时，其中一个同步副本将成为新首领（这里有一个例外，第 6 章中已经讨论过了）。
分区的首领肯定是同步副本，而对跟随者副本来说，则需要满足以下条件才能被认为是同步副本。

- 与 ZooKeeper 之间有一个活跃的会话，也就是说，它在过去的 6 秒（可配置）内向 ZooKeeper 发送过 心跳。 

- 在过去的 10 秒（可配置）内从首领那里复制过消息。

-  在过去的 10 秒内从首领那里复制过最新的消息。仅从首领那里复制消息是不够的，它还必须在每 10

  秒（可配置）内复制一次最新的消息。

如果跟随者副本不能满足以上任何一点（比如与 ZooKeeper 断开连接，或者不再复制新消息，或者复制消息滞后了 10 秒以上），那么它就会被认为是不同步的。一个不同步的副本可以通过与 ZooKeeper 重新建立连接并从首领那里复制最新的消息重新变成同步副本。如果网络出现了临时问题，并快速得到了修复，那么重新变成同步副本就很快。但如果副本所在的 broker 发生了崩溃，那么这个过程就需要较长时间。



### 不同步副本

一个稍有滞后的同步副本会导致生产者和消费者变慢，因为在消息被认为已提交之前，客户端会等待所有同步副本确认消息。如果一个副本变成不同步的，那么我们就不再关心它是否已经收到消息。这个时候，虽然不同步副本同样是滞后的，但它不影响性能。然而，更少的同步副本意味着更小的有效复制系数，因此在停机时丢失数据的风险就更大了。



## broker 配置

### 复制系数

主题级别的配置参数是 replication.factor。在 broker 级别，可以通过`default.replication.factor `来设置自动创建的主题的复制系数。

到目前为止，本书都是假设主题的复制系数是 3，也就是说每个分区总共会被 3 个不同的 broker 复制 3次。这样的假设是合理的，因为这就是 Kafka 默认的复制系数，不过用户可以修改它。即使是在主题被创建之后，仍然可以使用 Kafka 的副本分配工具新增或移除副本，以此来改变复制系数。

如果复制系数是 N，那么在 N–1 个 broker 失效的情况下，客户端仍然能够从主题读取数据或向主题写入数据。所以，更高的复制系数会带来更高的可用性、可靠性和更少的灾难性事故。另外，复制系数 N 需要至少 N 个 broker，也就是说我们会有 N 个数据副本，并且它们会占用 N 倍的磁盘空间。基本上，我们是在用硬件换取可用性。



### 不彻底的首领选举

unclean.leader.election.enable 只能在 broker 级别（实际上是在集群范围内）配置，它的默认 值是 false。 前面讲过，当分区的首领不可用时，一个同步副本将被选举为新首领。如果在选举过程中未丢失数据，也 就是说所有同步副本都包含了已提交的数据，那么这个选举就是“彻底”的。

总的来说，如果允许不同步副本成为首领，那么就要承担丢失数据和消费者读取到不一致的数据的风险。 如果不允许它们成为首领，那么就要接受较低的可用性，因为必须等待原先的首领恢复到可用状态。 

在默认情况下，unclean.leader.election.enable 的值是 false，也就是不允许不同步副本成为 首领。这是最安全的选项，因为它可以保证数据不丢失。这也意味着在之前描述的极端不可用场景中，一 些分区将一直不可用，直到手动恢复。当遇到这种情况时，管理员可以决定是否允许数据丢失，以便让分 区可用，如果可以，就在启动集群之前将其设置为 true，在集群恢复之后不要忘了再将其改回 false。





### 最少同步副本

`min.insync.replicas `参数可以配置在主题级别和 broker 级别。

如前所述，尽管为一个主题配置了 3 个副本，还是会出现只剩下一个同步副本的情况。如果这个同步副本变为不可用，则必须在可用性和一致性之间做出选择，而这是一个两难的选择。根据 Kafka 对可靠性保证的定义，一条消息只有在被写入所有同步副本之后才被认为是已提交的，但如果这里的“所有”只包含一个同步副本，那么当这个副本变为不可用时，数据就有可能丢失。
如果想确保已提交的数据被写入不止一个副本，就要把最少同步副本设置得大一些。对于一个包含 3 个副
本的主题，如果` min.insync.replicas` 被设置为 2，那么至少需要有两个同步副本才能向分区写入数
据。

如果 3 个副本都是同步的，那么一切正常进行。即使其中一个副本变为不可用，也不会有什么问题。但是，如果有两个副本变为不可用，那么 broker 就会停止接受生产者的请求。尝试发送数据的生产者会收到`NotEnoughReplicasException` 异常，不过消费者仍然可以继续读取已有的数据。实际上，如果使用这样的配置，那么当只剩下一个同步副本时，它就变成只读的了。这样做是为了避免在发生不彻底的选举时数据的写入和读取出现非预期的行为。要脱离这种只读状态，必须让两个不可用分区中的一个重新变为可用（比如重启 broker），并等待它变为同步的。



### 保持副本同步

前面提到过，不同步副本会降低总体可靠性，所以要尽量避免出现这种情况。一个副本可能在两种情况下变得不同步：要么它与 ZooKeeper 断开连接，要么它从首领复制消息滞后。对于这两种情况，Kafka 提供了两个 broker 端的配置参数。

zookeeper.session.timeout.ms 是允许 broker 不向 ZooKeeper 发送心跳的时间间隔。如果超过这个时间不发送心跳，则 ZooKeeper 会认为 broker 已经“死亡”，并将其从集群中移除。



### 持久化到磁盘

之前提到过，即使消息还没有被持久化到磁盘上，Kafka 也可以向生产者发出确认，这取决于已接收到消息的副本的数量。Kafka 会在重启之前和关闭日志片段（默认 1 GB 大小时关闭）时将消息冲刷到磁盘上，或者等到 Linux 系统页面缓存被填满时冲刷。其背后的想法是，拥有 3 台放置在不同机架或可用区域的机器，并在每台机器上放置一份数据副本比只将消息写入首领的磁盘更加安全，因为两个不同的机架或可用区域同时发生故障的可能性非常小。不过，也可以让 broker 更频繁地将消息持久化到磁盘上。配置参数 flush.messages 用于控制未同步到磁盘的最大消息数量，flash.ms 用于控制同步频率。在配置这些参数之前，最好先了解一下 fsync 是如何影响 Kafka 的吞吐量的以及如何尽量避开它的缺点。







## 在可靠的系统中使用消费者

消费者唯一要做的是跟踪哪些消息是已经读取过的，哪些消息是还未读取的，这是消费者在读取消息时不丢失消息的关键。

在从分区读取数据时，消费者会先获取一批消息，检查批次的最后一个偏移量，然后从这个偏移量开始读取下一批消息。这样可以保证消费者总能以正确的顺序获取新数据，不会错过任何消息。

如果一个消费者退出，那么另一个消费者需要知道从什么地方开始继续处理，以及前一个消费者在退出处理前的最后一个偏移量是多少。所谓的“另一个”消费者，也可能是原来的消费者重启之后重新上线。不过这个不重要，因为总归会有一个消费者继续从这个分区读取数据，重要的是它需要知道该从哪里开始读取。这也就是为什么消费者要“提交”偏移量。

消费者会把读取的每一个分区的偏移量都保存起来，这样在重启或其他消费者接手之后就知道从哪里开始读取了。**造成消费者丢失消息最主要的一种情况是它们提交了已读取消息的偏移量却未能全部处理完。在这种情况下，如果其他消费者接手了工作，那么那些没有被处理的消息就会被忽略，永远不会得到处理。这就是为什么我们非常重视何时以及如何提交偏移量。**



### 消费者的可靠性配置

为了保证消费者行为的可靠性，需要注意以下 4 个非常重要的配置参数。

第一个是 **group.id**，这个参数在第 4 章中已经详细介绍过了。如果两个消费者具有相同的群组 ID，并 订阅了同一个主题，那么每个消费者将分到主题分区的一个子集，也就是说它们只能读取到所有消息的一 个子集（但整个群组可以读取到主题所有的消息）。如果你希望一个消费者可以读取主题所有的消息，那么就需要为它设置唯一的 group.id。



第二个是 **auto.offset.reset**，这个参数指定了当没有偏移量（比如在消费者首次启动时）或请求的偏移量在 broker 上不存在时（第 4 章已经介绍过这种场景）消费者该作何处理。这个参数有两个值，一个是 earliest，如果配置了这个值，那么消费者将从分区的开始位置读取数据，即使它没有有效的偏移量。这会导致消费者读取大量的重复数据，但可以保证最少的数据丢失。另一个值是 latest，如果配置了这个值，那么消费者将从分区的末尾位置读取数据。这样可以减少重复处理消息，但很有可能会错过一些消息。



第三个是 **enable.auto.commit**，你可以决定让消费者自动提交偏移量，也可以在代码里手动提交偏移量。自动提交的一个最大好处是可以少操心一些事情。如果是在消费者的消息轮询里处理数据，那么自动提交可以确保不会意外提交未处理的偏移量。

`自动提交`的主要`缺点`是我们无法控制应用程序可能重复处理的消息的数量，比如在一次自动提交之前，消费者处理了一部分消息，但还没有提交偏移量就发生了崩溃或重启，那么这些消息会被重新处理（因为它们的偏移量没有被提交），这可能导致**消息的重复处理**。

如果应用程序的处理逻辑比较复杂（比如把消息交给另外一个后台线程去处理），那么就只能使用手动提交了，因为自动提交机制有可能会在还没有处理完消息时就提交偏移量。导致当消费者重启时，它认为这些消息已经被处理过了，而实际上它们还没有被完全处理。这就可能导致数据丢失或者不一致。



第四个配置参数 **auto.commit.interval.ms** 与第三个参数有直接的联系。如果选择使用自动提交，那么可以通过这个参数来控制提交的频率，默认每 5 秒提交一次。一般来说，频繁提交会增加额外的开销，但也会降低重复处理消息的概率。



如果一个消费者经常因为发生再均衡而暂停处理消息，则很难说它是可靠的，尽管这与数据处理的可靠性
没有直接关系。第 4 章提供过一些关于如何减少不必要的再均衡以及在再均衡期间减少消费者停顿的建
议。





### 手动提交偏移量

如果想要更大的灵活性，选择了手动提交，那么就需要考虑正确性和性能方面的问题。

**总是在处理完消息后提交偏移量**

如果所有的处理逻辑都是在轮询里进行的，并且不需要维护轮询之间的状态（比如为了聚合数据），那么就很简单。我们可以使用自动提交，在轮询结束时提交偏移量，也可以在轮询里提交偏移量，并选择一个合适的提交频率，在额外的开销和重复消息量之间取得平衡。



**提交频率是性能和重复消息数量之间的权衡**

即使是在最简单的场景中（比如所有的处理逻辑都在轮询里进行，并且不需要维护轮询之间的状
态），仍然可以选择在一个轮询里提交多次或多个轮询提交一次。提交偏移量需要额外的开销，这有点儿类似生产者配置了 acks=all，但同一个消费者群组提交的偏移量会被发送给同一个 broker，这可能会导致 broker 超载。提交频率需要在性能需求和重复消息量之间取得平衡。处理一条消息就提交一次偏移量的方式只适用于吞吐量非常低的主题。



 **在正确的时间点提交正确的偏移量**

在轮询过程中提交偏移量有一个缺点，就是有可能会意外提交已读取但未处理的消息的偏移量。一定 要在处理完消息后再提交偏移量，这点很关键——提交已读取但未处理的消息的偏移量会导致消费者 错过消息。



 **再均衡**

在设计应用程序时，需要考虑到消费者会发生再均衡并需要处理好它们。



**消费者可能需要重试**

有时候，在调用了轮询方法之后，有些消息需要稍后再处理。假设我们要把 Kafka 的数据写到数据库，但此时数据库不可用，那么就需要稍后再重试。需要注意的是，消费者提交偏移量并不是对单条消息的“确认”，这与传统的发布和订阅消息系统不一样。也就是说，如果记录 #30 处理失败，但记录#31 处理成功，那么就不应该提交记录 #31 的偏移量——如果提交了，就表示 #31 以内的记录都已处理完毕，包括记录 #30 在内，但这可能不是我们想要的结果。不过，可以采用以下两种模式来解决这个问题。

第一种模式，在遇到可重试错误时，提交最后一条处理成功的消息的偏移量，然后把还未处理好的消息保存到缓冲区（这样下一个轮询就不会把它们覆盖掉），并调用消费者的 pause() 方法，确保其他的轮询不会返回数据，之后继续处理缓冲区里的消息。

第二种模式，在遇到可重试错误时，把消息写到另一个重试主题，并继续处理其他消息。另一个消费者群组负责处理重试主题中的消息，或者让一个消费者同时订阅主主题和重试主题。这种模式有点儿像其他消息系统中的死信队列。





### 消息重试

在 Kafka 中，如果你需要控制消息的重试次数，需要**应用层自行维护消息的重试次数**，因为 Kafka 本身并没有内置的重试计数机制。以下是常见的方式来实现这一需求：



#### **在消息中附加重试次数**：

一种常见的做法是，给每条消息附加一个字段，用于记录它已经被处理失败并重试的次数。具体步骤如下：

- **步骤**：

  1. 在生产者发送消息时，消息的内容里可以附加一个属性（如 `retryCount`），初始值为 0。
  2. 消费者在处理消息时，每次处理失败就增加该 `retryCount` 的值。
  3. 在达到预设的重试次数（例如 3 次）之后，判断该消息为无法处理，最终将其发送到死信队列（DLQ）或者持久化到数据中（进行消息补偿）。

- **优点**：重试次数的信息和消息本身绑定，能够很好地跟踪每条消息的重试情况。

- **示例**（伪代码）：

  ```
  int retryCount = message.getRetryCount();
  if (retryCount < MAX_RETRY_COUNT) {
      try {
          // 处理消息
      } catch (Exception e) {
          retryCount++;
          message.setRetryCount(retryCount);
          // 重新发送回到 Kafka，或放入等待队列重试
      }
  } else {
      // 将消息发送到死信队列或者持久化到数据中
      deadLetterProducer.send(message);
  }
  ```





#### **使用 Kafka Streams 或外部框架**：

一些 Kafka 框架（如 Kafka Streams 或 Spring Kafka）提供了处理重试逻辑的内置功能，可以通过配置项实现自动的重试和死信队列机制。比如 Spring Kafka 的 `SeekToCurrentErrorHandler` 可以在消费失败时进行重试，并将超限的消息发送到死信队列。

>这里，我们来简单说说 SeekToCurrentErrorHandler 是怎么提供消费重试的功能的。
>
>- 在消息消费失败时，SeekToCurrentErrorHandler 会将 调用 Kafka Consumer 的 #seek(TopicPartition partition, long offset) 方法，将 Consumer 对于该消息对应的 TopicPartition 分区的本地进度设置成该消息的位置。这样，Consumer 在下次从 Kafka Broker 拉取消息的时候，又能重新拉取到这条消费失败的消息，并且是第一条。
>
>- 同时，Spring-Kafka 使用 FailedRecordTracker 对每个 Topic 的每个 TopicPartition 消费失败次数进行计数，这样相当于对该 TopicPartition 的第一条消费失败的消息的消费失败次数进行计数。😈 这里，胖友好好思考下，结合艿艿在上一点的描述。
>
>- 另外，在 FailedRecordTracker 中，会调用 BackOff 来进行计算，该消息的下一次重新消费的时间，通过 Thread#sleep(...) 方法，实现重新消费的时间间隔。
>
>- 有一点需要注意，FailedRecordTracker 提供的计数是**客户端**级别的，重启 JVM 应用后，计数是会丢失的。所以，如果想要计数进行持久化，需要自己重新实现下 FailedRecordTracker 类，通过 ZooKeeper 存储计数。
>
>  > 😈 RocketMQ 提供的消费重试的计数，目前是**服务端**级别，已经进行持久化。





#### 总结：

- Kafka 本身不提供重试次数的管理，需要在应用层维护。
- 可以通过**消息属性**或**外部存储**来记录重试次数，并结合死信队列实现消息的重试与错误处理。
- 也可以使用 Kafka 的延迟队列来增加重试间隔，避免频繁重试。

这取决于你的业务需求和系统复杂性，选择合适的方式来实现。





## 在可靠的系统中使用生产者

即使我们会尽可能地把 broker 配置得很可靠，但如果没有对生产者进行可靠性方面的配置，则整个系统仍然存在丢失数据的风险。

请看下面的两个例子。

- 我们为 broker 配置了 3 个副本，**并禁用了不彻底的首领选举**，这样应该可以保证已提交的消息不会丢失。不过，我们把生产者发送消息的 **acks 设置成了 1**。生产者向首领发送了一条消息，虽然其被首领成功写入，但其他同步副本还没有收到这条消息。首领向生产者发送了一个响应，告诉它“消息写入成功”，然后发生了崩溃，而此时其他副本还没有复制这条消息。另外两个副本此时仍然被认为是同步的，并且其中的一个副本会成为新首领。因为消息还没有被写入这两个副本，所以就丢失了，但发送消息的客户端认为消息已经成功写入。从消费者的角度来看，系统仍然是一致的，因为它们看不到丢失的消息（副本没有收到这条消息，不算已提交），但从生产者的角度来看，这条消息丢失了。
- 我们为 broker 配置了 3 个副本，**并禁用了不彻底的首领选举**。我们接受了之前的教训，把生产者的**acks 设置成了 all**。假设现在生产者向 Kafka 发送了一条消息，此时分区首领刚好发生崩溃，新首领在选举当中，Kafka 会向生产者返回“首领不可用”的响应。在这个时候，如果生产者未能正确处理这个异常，也没有重试发送消息，那么消息也有可能丢失。这不算是 broker 的可靠性问题，因为broker 并没有收到这条消息；这也不是一致性问题，因为消费者也不会读取到这条消息。问题在于，如果生产者未能正确处理异常，就有可能丢失数据。


从上面的两个例子可以看出，开发人员需要注意两件事情。 根据可靠性需求配置恰当的 acks。 正确配置参数，并在代码里正确处理异常。



### 发送确认

生产者可以选择以下 3 种确认模式。

**acks=0**

 如果生产者能够通过网络把消息发送出去，那么就认为消息已成功写入 Kafka。不过，在这种情况下仍然有可能出现错误，比如发送的消息对象无法被序列化或者网卡发生故障。如果此时分区离线、正在进行首领选举或整个集群长时间不可用，则并不会收到任何错误。在 acks=0 模式下，生产延迟是很低的（这就是为什么很多基准测试是基于这种模式的），但它对端到端延迟并不会带来任何改进（在消息被所有可用副本复制之前，消费者是看不到它们的）。

**acks=1**

首领在收到消息并把它写入分区数据文件（不一定要冲刷到磁盘上）时会返回确认或错误响应。在这种模式下，如果首领被关闭或发生崩溃，那么那些已经成功写入并确认但还没有被跟随者复制的消息就丢失了。另外，消息写入首领的速度可能比副本从首领那里复制消息的速度更快，这样会导致分区复制不及时，因为首领在消息被副本复制之前就向生产者发送了确认响应。

**acks=all**

首领在返回确认或错误响应之前，会等待所有同步副本都收到消息。这个配置可以和min.insync.replicas 参数结合起来，用于控制在返回确认响应前至少要有多少个副本收到消息。这是最安全的选项，因为生产者会一直重试，直到消息提交成功。不过，这种模式下的生产者延迟也最大，因为生产者在发送下一批次消息之前需要等待所有副本都收到当前批次的消息。



### 配置生产者的重试参数

生产者需要处理的错误包括两个部分：**一部分是由生产者自动处理的错误，另一部分是需要开发者手动处理的错误**。
生产者可以自动处理可重试的错误。当生产者向 broker 发送消息时，broker 可以返回一个成功响应或者错
误响应。`错误响应`可以分为两种，**一种是在重试之后可以解决的**，**另一种是无法通过重试解决的**。如果broker 返回 `LEADER_NOT_AVAILABLE 错误`，那么生产者可以尝试重新发送消息——或许新首领被选举出来了，那么第二次尝试发送就会成功。**也就是说，LEADER_NOT_AVAILABLE 是一个可重试错误**。如果 broker 返回` INVALID_CONFIG 错误`，那么即使重试发送消息也无法解决这个问题，所以这样的重试是没有意义的，**这是不可重试错误**。
一般来说，**如果你的目标是不丢失消息，那么就让生产者在遇到可重试错误时保持重试**。正如第 3 章所建
议的那样，**最好的重试方式是使用默认的重试次数（整型最大值或无限），并把delivery.timeout.ms 配置成我们愿意等待的时长，生产者会在这个时间间隔内一直尝试发送消息**。

`重试发送消息`存在一定的风险，因为如果两条消息都成功写入，则会导致`消息重复`。通过重试和小心翼翼地处理异常，可以保证每一条消息都会被保存至少一次，但不能保证只保存一次。如果把`enable.idempotence `参数设置为 true，那么生产者就会在消息里加入一些额外的信息，broker 可以使用这些信息来跳过因重试导致的重复消息。





### 额外的错误处理

使用生产者内置的重试机制可以在不造成消息丢失的情况下轻松地处理大部分错误，但开发人员仍然需要
处理以下这些其他类型的错误。

- 不可重试的 broker 错误，比如消息大小错误、身份验证错误等。
- 在将消息发送给 broker 之前发生的错误，比如序列化错误。
- 在生产者达到重试次数上限或重试消息占用的内存达到上限时发生的错误。
- 超时。

第 3 章介绍过如何在同步发送和异步发送消息时处理错误。**这些错误的处理逻辑与具体的应用程序及其目标有关**——**丢弃“不合法的消息”**？**把错误记录下来**？**停止从源系统读取消息**？**对源系统应用回压策略以便暂停发送消息**？**把消息保存到本地磁盘的某个目录里**？具体使用哪一种逻辑要**根据实际的架构和产品需求来决定**。**只需记住，如果错误处理只是为了重试发送消息，那么最好还是使用生产者内置的重试机制**。

>注：具体使用哪一种逻辑要**根据实际的架构和产品需求来决定**。